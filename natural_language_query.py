#!/usr/bin/env python3
"""
Natural Language Query Tool for Spending Data
Uses local LLM (Ollama) - no API required
"""

from spending_lm import SpendingLM
import sys
import os

def print_banner():
    """Print welcome banner"""
    print("\n" + "="*70)
    print("ğŸ’° NATURAL LANGUAGE SPENDING ANALYZER")
    print("="*70)
    print("Query your spending data using plain English!")
    print("Running completely locally - no external APIs needed")
    print("="*70 + "\n")

def quick_start():
    """Print quick start instructions"""
    print("""
QUICK START:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. FIRST TIME SETUP:
   
   # Download a model (one-time, takes 5-10 minutes)
   python3 natural_language_query.py --download

2. START OLLAMA SERVER:
   
   # In a new terminal:
   ollama serve

3. INTERACTIVE QUERIES:
   
   # Ask questions about your spending
   python3 natural_language_query.py

EXAMPLES OF QUESTIONS YOU CAN ASK:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ "How much did I spend on education?"
  âœ“ "What was my highest spending category last month?"
  âœ“ "How many transactions were over $200?"
  âœ“ "Compare my shopping vs restaurant spending"
  âœ“ "What percentage of my budget went to utilities?"
  âœ“ "Show me all transactions categorized as entertainment"
  âœ“ "Analyze my spending patterns and suggest areas to save"

COMMAND LINE OPTIONS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  python3 natural_language_query.py
    â””â”€ Interactive mode (ask multiple questions)
  
  python3 natural_language_query.py "How much on groceries?"
    â””â”€ Single query mode
  
  python3 natural_language_query.py --analyze
    â””â”€ Generate automatic spending analysis
  
  python3 natural_language_query.py --download
    â””â”€ Download the Mistral model
  
  python3 natural_language_query.py --list-models
    â””â”€ Show installed models
  
  python3 natural_language_query.py --model llama2 "question"
    â””â”€ Use a different model

MODELS AVAILABLE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â€¢ llama2 (7B, fast, recommended) â­
  â€¢ llama2 (7GB, slower, more powerful)
  â€¢ neural-chat (4GB, optimized for chat)
  â€¢ dolphin-mixtral (26GB, very powerful)

REQUIREMENTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ Ollama installed (via: brew install ollama)
  âœ“ Python 3.7+
  âœ“ requests library (auto-installed)
  âœ“ spending data files in current directory

GETTING HELP:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  python3 natural_language_query.py --help
    â””â”€ Show all options

""")

def check_ollama_setup():
    """Check if Ollama is properly set up"""
    import requests
    
    try:
        # Check if Ollama is running
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            data = response.json()
            models = [m["name"].split(":")[0] for m in data.get("models", [])]
            
            if not models:
                print("âš ï¸  Ollama is running but no models found")
                print("   Download a model with: ollama pull llama2")
                return False
            
            if "llama2" not in models:
                print(f"âš ï¸  Found models: {', '.join(set(models))}")
                print(f"   Recommended: ollama pull llama2")
                return False
            
            return True
        else:
            print("âš ï¸  Ollama server not responding properly")
            return False
    except requests.exceptions.ConnectionError:
        print("âš ï¸  Ollama is not running!")
        print("   Start it with: ollama serve")
        return False
    except Exception as e:
        print(f"âš ï¸  Error checking Ollama: {e}")
        return False

def main():
    print_banner()
    
    # Check Ollama setup
    if not check_ollama_setup():
        print("\nâŒ Ollama setup incomplete. Fix the issues above and try again.\n")
        return
    
    print("âœ… Ollama and model ready!\n")
    
    # Check if this is first time
    if len(sys.argv) == 1:
        print("â„¹ï¸  Starting in interactive mode...")
        print("(For help, run: python3 natural_language_query.py --help)\n")
        
        lm = SpendingLM()
        lm.load_spending_data()
        lm.interactive_session()
    else:
        # Pass through to spending_lm
        from spending_lm import main
        main()

if __name__ == "__main__":
    main()
